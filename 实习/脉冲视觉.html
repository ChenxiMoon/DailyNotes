<!doctype html>
<html lang="zh-CN">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width,initial-scale=1" />
    <meta name="generator" content="VuePress 2.0.0-rc.24" />
    <style>
      :root {
        --vp-c-bg: #fff;
      }

      [data-theme='dark'] {
        --vp-c-bg: #1b1b1f;
      }

      html,
      body {
        background-color: var(--vp-c-bg);
      }
    </style>
    <script>
      const useChoice = localStorage.getItem('vuepress-color-scheme')
      const systemStatus =
        'matchMedia' in window
          ? window.matchMedia('(prefers-color-scheme: dark)').matches
          : false

      if (useChoice === 'light') {
        document.documentElement.dataset.theme = 'light'
      } else if (useChoice === 'dark' || systemStatus) {
        document.documentElement.dataset.theme = 'dark'
      }
    </script>
    <title>三个问题 | 我的笔记</title><meta name="description" content="每日学习记录">
    <link rel="preload" href="/assets/style-C2JCxPSn.css" as="style"><link rel="stylesheet" href="/assets/style-C2JCxPSn.css">
    <link rel="modulepreload" href="/assets/app-DIN4ixgB.js"><link rel="modulepreload" href="/assets/脉冲视觉.html-r7aLJh3j.js">
    <link rel="prefetch" href="/assets/index.html-C5e511lc.js" as="script"><link rel="prefetch" href="/assets/我是DailyNotes.html-D182c_CL.js" as="script"><link rel="prefetch" href="/assets/我是新创建的文件 试一下本地创建文件会上传github吗.html-BZB5CO0d.js" as="script"><link rel="prefetch" href="/assets/英语.html-DqFJsEkj.js" as="script"><link rel="prefetch" href="/assets/404.html-CFd7g205.js" as="script">
  </head>
  <body>
    <div id="app"><!--[--><div class="vp-theme-container no-sidebar external-link-icon" vp-container><!--[--><header class="vp-navbar" vp-navbar><div class="vp-toggle-sidebar-button" title="toggle sidebar" aria-expanded="false" role="button" tabindex="0"><div class="icon" aria-hidden="true"><span></span><span></span><span></span></div></div><span><a class="route-link" href="/"><!----><span class="vp-site-name" aria-hidden="true">我的笔记</span></a></span><div class="vp-navbar-items-wrapper" style=""><!--[--><!--]--><nav class="vp-navbar-items vp-hide-mobile" aria-label="site navigation"><!--[--><div class="vp-navbar-item"><a class="route-link auto-link" href="/" aria-label="首页"><!--[--><!--[--><!--]--><!--]-->首页<!--[--><!--[--><!--]--><!--]--></a></div><div class="vp-navbar-item"><a class="route-link auto-link" href="/about/" aria-label="关于"><!--[--><!--[--><!--]--><!--]-->关于<!--[--><!--[--><!--]--><!--]--></a></div><!--]--></nav><!--[--><!--]--><button type="button" class="vp-toggle-color-mode-button" title="toggle color mode"><svg class="light-icon" viewbox="0 0 32 32" style=""><path d="M16 12.005a4 4 0 1 1-4 4a4.005 4.005 0 0 1 4-4m0-2a6 6 0 1 0 6 6a6 6 0 0 0-6-6z" fill="currentColor"></path><path d="M5.394 6.813l1.414-1.415l3.506 3.506L8.9 10.318z" fill="currentColor"></path><path d="M2 15.005h5v2H2z" fill="currentColor"></path><path d="M5.394 25.197L8.9 21.691l1.414 1.415l-3.506 3.505z" fill="currentColor"></path><path d="M15 25.005h2v5h-2z" fill="currentColor"></path><path d="M21.687 23.106l1.414-1.415l3.506 3.506l-1.414 1.414z" fill="currentColor"></path><path d="M25 15.005h5v2h-5z" fill="currentColor"></path><path d="M21.687 8.904l3.506-3.506l1.414 1.415l-3.506 3.505z" fill="currentColor"></path><path d="M15 2.005h2v5h-2z" fill="currentColor"></path></svg><svg class="dark-icon" viewbox="0 0 32 32" style="display:none;"><path d="M13.502 5.414a15.075 15.075 0 0 0 11.594 18.194a11.113 11.113 0 0 1-7.975 3.39c-.138 0-.278.005-.418 0a11.094 11.094 0 0 1-3.2-21.584M14.98 3a1.002 1.002 0 0 0-.175.016a13.096 13.096 0 0 0 1.825 25.981c.164.006.328 0 .49 0a13.072 13.072 0 0 0 10.703-5.555a1.01 1.01 0 0 0-.783-1.565A13.08 13.08 0 0 1 15.89 4.38A1.015 1.015 0 0 0 14.98 3z" fill="currentColor"></path></svg></button><!----></div></header><!--]--><div class="vp-sidebar-mask"></div><!--[--><aside class="vp-sidebar" vp-sidebar><nav class="vp-navbar-items" aria-label="site navigation"><!--[--><div class="vp-navbar-item"><a class="route-link auto-link" href="/" aria-label="首页"><!--[--><!--[--><!--]--><!--]-->首页<!--[--><!--[--><!--]--><!--]--></a></div><div class="vp-navbar-item"><a class="route-link auto-link" href="/about/" aria-label="关于"><!--[--><!--[--><!--]--><!--]-->关于<!--[--><!--[--><!--]--><!--]--></a></div><!--]--></nav><!--[--><!--]--><!----><!--[--><!--]--></aside><!--]--><!--[--><main class="vp-page"><!--[--><!--]--><div vp-content><!--[--><!--]--><div id="content"><h1 id="三个问题" tabindex="-1"><a class="header-anchor" href="#三个问题"><span>三个问题</span></a></h1><h2 id="脉冲视觉的工作原理" tabindex="-1"><a class="header-anchor" href="#脉冲视觉的工作原理"><span>脉冲视觉的工作原理？</span></a></h2><ol><li>脉冲视觉（Spike Vision）是由北京大学黄铁军教授的团队提出的一种全新的视觉信息表达体系，旨在克服传统视频成像的局限性 。它的核心原理是模拟生物视觉系统，通过一种“光子累积和阈值触发”的机制来捕捉和编码视觉信息</li></ol><h3 id="脉冲视觉研究进展-发表在中国图像图形学报" tabindex="-1"><a class="header-anchor" href="#脉冲视觉研究进展-发表在中国图像图形学报"><span>脉冲视觉研究进展 发表在中国图像图形学报</span></a></h3><h3 id="spikecv-open-a-continuous-computer-vision-era" tabindex="-1"><a class="header-anchor" href="#spikecv-open-a-continuous-computer-vision-era"><span>SpikeCV: Open a Continuous Computer Vision Era</span></a></h3><h3 id="脉冲视觉的工作原理是对传统摄像机基于-帧-的成像模式的颠覆-。它不采用固定的曝光时间和同步快门-而是模拟生物视网膜的感光方式-其核心原理是" tabindex="-1"><a class="header-anchor" href="#脉冲视觉的工作原理是对传统摄像机基于-帧-的成像模式的颠覆-。它不采用固定的曝光时间和同步快门-而是模拟生物视网膜的感光方式-其核心原理是"><span>脉冲视觉的工作原理是对传统摄像机基于“帧”的成像模式的颠覆 。它不采用固定的曝光时间和同步快门，而是模拟生物视网膜的感光方式 ，其核心原理是：</span></a></h3><p>持续捕获光子，异步产生脉冲：脉冲视觉的感光器件是持续工作的，它不断捕获并累积光子能量 。</p><p>达到阈值即发放：当某个感光单元累积的光子能量达到了一个预先设定的阈值时，它就会瞬间产生一个脉冲（或称为“事件”），然后能量清零，重新开始下一次的累积 。</p><p>脉冲时间编码光强：脉冲产生的时间间隔蕴含了光照的强度信息 。光线越强，能量累积得越快，达到阈值产生脉冲的时间就越短；反之，光线越弱，产生脉冲所需的时间就越长 。</p><p>简单来说，传统相机像是每隔一段时间（例如 1/30 秒）集体拍一张合影 ，而脉冲相机则像是每个感光单元有了光强变化就立刻“举手”报告一次，完全不受“帧率”的限制 。这种机制使得脉冲视觉能够以极高的时间分辨率来记录光的变化过程 。</p><h3 id="详细原理和与传统视频的区别" tabindex="-1"><a class="header-anchor" href="#详细原理和与传统视频的区别"><span>详细原理和与传统视频的区别</span></a></h3><ol><li>感光和脉冲生成机制：</li></ol><ul><li><p><strong>传统视频：</strong> 传统相机的成像模型是所有感光单元在预定的帧率下（例如几十 Hz），同步进行持续时长相同的曝光，然后将累积的光强记录下来，形成一帧图像 。这种方式会丢失两次曝光之间所有光线的变化过程 。</p></li><li><p><strong>脉冲视觉：</strong> 脉冲视觉模型中的感光器件会<strong>持续</strong>捕获光子 。当累积的光强超过一个预设的</p><p><strong>阈值</strong>时，它就会<strong>异步</strong>地产生一个脉冲 。这种产生脉冲和其持续时长的过程，被称为一个**“视元”**</p></li></ul><ol start="2"><li>信息编码方式：</li></ol><ul><li><strong>传统视频：</strong> 图像通过记录曝光时间内的平均光强来表达视觉信息 。这导致图像无法记录光线变化的过程 。</li><li><strong>脉冲视觉：</strong> 脉冲视觉将光强信息编码在了脉冲的<strong>时间间隔</strong>中 。 <ul><li>脉冲生成的时间越长，表明接收到的光信号越弱 。</li><li>反之，脉冲生成的时间越短，光信号就越强 。</li><li>这种方式使得脉冲流具有清晰的物理意义，可以用来估计任意时刻的光强 。</li></ul></li></ul><ol start="3"><li>时域采样和“全时成像”：</li></ol><ul><li><p><strong>传统视频：</strong> 视频的时域采样率是人为规定的，比如数十 Hz，且两帧图像之间的信息是完全丢失的 。</p></li><li><p><strong>脉冲视觉：</strong> 脉冲相机不采用同步曝光，而是持续捕获光子 。每个感光器件产生的脉冲按照时间次序排成序列，所有感光器件的脉冲序列组成</p></li><li><p><strong>脉冲流阵列</strong>，这就是脉冲视觉的表达形式——<strong>“视达（Vidar）”</strong>，用以代替传统的视频（Video） 。</p></li><li><p>由于视达有效保留了各个采样位置光线的时域变化过程，可以从脉冲流中重构出</p><p><strong>任意时刻的画面</strong> 。这种能力被称为**“全时成像”（fulltime imaging）**或连续成像（continuous imaging）</p></li></ul><h3 id="脉冲视觉的突出优势" tabindex="-1"><a class="header-anchor" href="#脉冲视觉的突出优势"><span>脉冲视觉的突出优势</span></a></h3><ul><li><strong>超高速：</strong> 脉冲视觉可以实现千万 Hz 的高速时域采样，能够捕捉传统相机无法记录的极快物理过程 。例如，北京大学开发的芯片已能清晰拍摄时速 350km 的高铁和每分钟 7200 转的硬盘 。</li><li><strong>全时：</strong> 脉冲流蕴含了任意时刻的影像能力，可以重构出连续的画面，就像人脑对外部环境的连续感知一样 。</li><li><strong>高动态范围：</strong> 脉冲视觉通过积分采样原理，可以对场景中亮处和暗处的信息都能进行保留，为实现高动态范围成像提供了可能 。</li><li><strong>宽频成像</strong>： 利用相机的高频能力，可以获得各像素的振动频率，从而区分不同目标（如螺旋桨、抖动、海浪等） 。</li></ul><h2 id="脉冲是怎么成像的" tabindex="-1"><a class="header-anchor" href="#脉冲是怎么成像的"><span>脉冲是怎么成像的？</span></a></h2><h3 id="脉冲本身并不是图像-而是包含了光强和时间信息的数据流-称为-视达-vidar-。-从这种脉冲流中重建出我们所见的图像-即-成像-主要依赖于对脉冲时间的分析和计算。" tabindex="-1"><a class="header-anchor" href="#脉冲本身并不是图像-而是包含了光强和时间信息的数据流-称为-视达-vidar-。-从这种脉冲流中重建出我们所见的图像-即-成像-主要依赖于对脉冲时间的分析和计算。"><span>脉冲本身并不是图像，而是包含了光强和时间信息的数据流，称为“视达 (Vidar)” 。 从这种脉冲流中重建出我们所见的图像（即“成像”），主要依赖于对脉冲时间的分析和计算。</span></a></h3><h3 id="成像的过程被称为-脉冲影像重建-论文中提到了几种主要方法" tabindex="-1"><a class="header-anchor" href="#成像的过程被称为-脉冲影像重建-论文中提到了几种主要方法"><span>成像的过程被称为“脉冲影像重建” ，论文中提到了几种主要方法：</span></a></h3><h4 id="全时成像-full-time-imaging-这是脉冲视觉最核心的成像能力-。因为每个脉冲的持续时间都与光线强度成反比-所以理论上可以根据这个关系估计出任意时刻的光强值-从而重建出任意时刻的精细图像-。" tabindex="-1"><a class="header-anchor" href="#全时成像-full-time-imaging-这是脉冲视觉最核心的成像能力-。因为每个脉冲的持续时间都与光线强度成反比-所以理论上可以根据这个关系估计出任意时刻的光强值-从而重建出任意时刻的精细图像-。"><span>全时成像 (Full-time Imaging)：这是脉冲视觉最核心的成像能力 。因为每个脉冲的持续时间都与光线强度成反比，所以理论上可以根据这个关系估计出任意时刻的光强值，从而重建出任意时刻的精细图像 。</span></a></h4><h3 id="基础重建算法" tabindex="-1"><a class="header-anchor" href="#基础重建算法"><span>基础重建算法：</span></a></h3><p>基于脉冲间隔法 (TFI)：直接利用“脉冲间隔越短，光强越强”这一特性来计算瞬时的光强，但这种方法容易受光子波动影响，产生噪声 。</p><p>基于时间窗平均法 (TFW)：在一定时间窗口内统计脉冲的数量来计算平均光强 。这种方法对静态场景成像更稳定，但如果物体在高速运动，则会产生运动模糊 。</p><p>高级重建算法：为了克服基础算法的缺点，研究人员提出了更复杂的算法 。例如，通过运动补偿对脉冲数据进行滤波，可以在提升信噪比的同时避免运动模糊 ；或者利用脉冲神经网络模型来提升图像的对比度 。</p><h3 id="总结来说-脉冲成像是通过解码每个像素点产生的脉冲序列的时间信息-反推出该点在任意时刻的光照强度-最终将所有像素点的光强分布组合起来-形成一幅完整的图像。" tabindex="-1"><a class="header-anchor" href="#总结来说-脉冲成像是通过解码每个像素点产生的脉冲序列的时间信息-反推出该点在任意时刻的光照强度-最终将所有像素点的光强分布组合起来-形成一幅完整的图像。"><span>总结来说，脉冲成像是通过解码每个像素点产生的脉冲序列的时间信息，反推出该点在任意时刻的光照强度，最终将所有像素点的光强分布组合起来，形成一幅完整的图像。</span></a></h3><h2 id="讲出三种脉冲成像的算法以及区别" tabindex="-1"><a class="header-anchor" href="#讲出三种脉冲成像的算法以及区别"><span>讲出三种脉冲成像的算法以及区别</span></a></h2><h3 id="_1️⃣-tfp-temporal-filtering-processing-时域滤波重建" tabindex="-1"><a class="header-anchor" href="#_1️⃣-tfp-temporal-filtering-processing-时域滤波重建"><span>1️⃣ TFP（Temporal Filtering Processing / 时域滤波重建）</span></a></h3><p>核心原理</p><p>输入：脉冲相机输出的是一个事件流，每个事件包含像素位置 (x, y)、时间戳 t 和极性 p（亮度增加或减少）。</p><p>处理：TFP 在每个像素上建立一个时间滤波器（通常是指数衰减滤波或滑动窗口滤波），将事件的时间分布转化为强度值。</p><p>重建：通过滤波器的输出对像素灰度进行更新，生成当前时刻的重建图像。</p><p>关键公式（指数衰减型）：</p><p><img src="/assets/image-BkIz5K8n.png" alt="alt text"></p><p>其中：</p><p>τ 是时间常数，控制响应速度与平滑度</p><p>p i 是事件极性</p><p>优点</p><p>计算简单，速度快</p><p>实时性好（适合在线重建）</p><p>缺点</p><p>只利用了时间维度信息，空间结构利用不足</p><p>容易受到局部噪声影响</p><h3 id="_2️⃣-tfstp-temporal-filtering-with-spatio-temporal-processing-时空滤波重建" tabindex="-1"><a class="header-anchor" href="#_2️⃣-tfstp-temporal-filtering-with-spatio-temporal-processing-时空滤波重建"><span>2️⃣ TFSTP（Temporal Filtering with Spatio-Temporal Processing / 时空滤波重建）</span></a></h3><p>核心原理</p><p>继承了 TFP 的时间滤波思想</p><p>额外引入空间邻域信息：在每次时间滤波的结果基础上，加入空间滤波（如卷积、高斯平滑、边缘增强），利用相邻像素的事件分布来抑制噪声、补充缺失信息。</p><p>相当于在时间滤波器后面再接一个空间滤波器，形成时空联合处理。</p><p>关键处理流程：</p><p>时间滤波：同 TFP</p><p>空间处理：对滤波结果进行空间卷积，权重可以固定（高斯核）或自适应（边缘保护滤波）</p><p>融合：得到噪声抑制且细节增强的重建图像</p><p>优点</p><p>利用空间上下文，抗噪性更强</p><p>细节恢复效果比 TFP 好</p><p>缺点</p><p>算法复杂度增加</p><p>实时性能略低于 TFP（尤其是空间核较大时）</p><h3 id="_3️⃣-ssml-spike-stream-machine-learning-深度学习重建" tabindex="-1"><a class="header-anchor" href="#_3️⃣-ssml-spike-stream-machine-learning-深度学习重建"><span>3️⃣ SSML（Spike Stream Machine Learning / 深度学习重建）</span></a></h3><p>核心原理</p><p>使用神经网络（通常是 CNN、U-Net、Transformer）直接学习事件流 → 图像的映射关系</p><p>输入可以是：</p><p>累积事件图（事件计数图）</p><p>时间表征图（time surface）</p><p>多通道事件张量（多时间窗口堆叠）</p><p>网络会学习到复杂的时空特征，不需要手工设计滤波器</p><p>输出是高质量的重建图像（可能还包含去噪、超分、颜色恢复等功能）</p><p>训练方式</p><p>有监督：用真实帧图像作为标签</p><p>自监督：利用事件数据的一致性约束（如重投影误差）训练</p><p>优点</p><p>能处理复杂运动、光照变化和噪声</p><p>图像质量通常优于传统滤波方法</p><p>缺点</p><p>需要大量训练数据</p><p>推理速度受模型规模影响</p><p>可能对训练集分布外的数据泛化差</p><h2 id="区别" tabindex="-1"><a class="header-anchor" href="#区别"><span>区别</span></a></h2><table><thead><tr><th>特性</th><th><strong>TFP</strong></th><th><strong>TFSTP</strong></th><th><strong>SSML</strong></th></tr></thead><tbody><tr><td><strong>类型</strong></td><td>传统时域滤波</td><td>时空联合滤波</td><td>深度学习</td></tr><tr><td><strong>利用信息</strong></td><td>时间</td><td>时间 + 空间</td><td>时间 + 空间（深度特征）</td></tr><tr><td><strong>计算复杂度</strong></td><td>低</td><td>中</td><td>高</td></tr><tr><td><strong>实时性</strong></td><td>很好</td><td>较好</td><td>依赖硬件</td></tr><tr><td><strong>抗噪性</strong></td><td>一般</td><td>好</td><td>很好</td></tr><tr><td><strong>细节恢复</strong></td><td>一般</td><td>好</td><td>最好</td></tr><tr><td><strong>依赖训练</strong></td><td>否</td><td>否</td><td>是</td></tr><tr><td><strong>典型应用</strong></td><td>实时监控、低延迟任务</td><td>噪声环境成像</td><td>高精度成像、科研拍摄</td></tr></tbody></table><p>脉冲成像（Spike Imaging）常用的三类典型算法主要是围绕 如何从脉冲序列（spike stream）恢复高质量图像 展开的，不同算法在建模思路、利用信息的方式、以及计算复杂度上有区别。常见三种方法及区别如下：</p><ol><li>累积重建（Integration-based Reconstruction） 原理</li></ol><p>将脉冲序列在时间维度上进行积分（累加），利用脉冲发射的频率与光强成正比的特性来恢复图像。</p><p>通常会在固定时间窗内统计每个像素的脉冲数量，直接映射为灰度值。</p><p>特点</p><p>优点：简单高效，实时性强（可以直接流式处理）。</p><p>缺点：容易产生运动模糊，时间窗大小选取敏感；噪声脉冲会直接影响图像质量。</p><p>典型场景</p><p>高速运动场景的粗略成像、低延迟目标检测。</p><ol start="2"><li>事件驱动滤波重建（Event-driven Filtering / Spike-based Filtering） 原理</li></ol><p>在脉冲信号的时间序列上应用滤波器（如指数衰减滤波、时空高斯滤波等）进行平滑与去噪。</p><p>通过滤波器的响应计算像素的瞬时光强，减少噪声并保留细节。</p><p>特点</p><p>优点：能在保持高时间分辨率的同时，抑制噪声。</p><p>缺点：滤波器参数（时间常数、空间核大小）需要精心调节；计算量比简单累积大。</p><p>典型场景</p><p>动态范围大、噪声较多的环境；需要较高空间清晰度的场景。</p><ol start="3"><li>深度学习重建（Deep Learning-based Spike Reconstruction） 原理</li></ol><p>将脉冲序列（可按时间片分割）输入神经网络（如 UNet、Spiking Neural Network、Transformer 等）进行端到端的成像重建。</p><p>网络会自动学习时空特征映射关系，将稀疏的二值脉冲转为连续高质量图像。</p><p>特点</p><p>优点：重建质量高，能够恢复纹理和细节，抗噪能力强。</p><p>缺点：需要大量标注数据训练；推理速度可能较慢，硬件要求高。</p><p>典型场景</p><p>超高速高质量成像、科研实验、需要恢复细节的视觉任务。</p><h3 id="好的老板-我们来深入探讨一下您提到的这7种算法的原理和它们之间的区别。" tabindex="-1"><a class="header-anchor" href="#好的老板-我们来深入探讨一下您提到的这7种算法的原理和它们之间的区别。"><span>好的老板，我们来深入探讨一下您提到的这7种算法的原理和它们之间的区别。</span></a></h3><p>这些算法可以分为三大类：<strong>图像重构</strong>、<strong>光流估计</strong>和<strong>多目标跟踪</strong>。</p><hr><h3 id="_1-图像-纹理重构算法-5个" tabindex="-1"><a class="header-anchor" href="#_1-图像-纹理重构算法-5个"><span>1. 图像/纹理重构算法 (5个)</span></a></h3><p>这类算法的核心目标是解决同一个问题：<strong>如何将稀疏、二值的脉冲信号转换成我们肉眼可见的、连续灰度的图像或视频</strong>。它们的主要区别在于实现这一目标的思路和技术手段。</p><table><thead><tr><th>算法</th><th>核心原理</th><th>主要区别与特点</th></tr></thead><tbody><tr><td><strong>TFP</strong></td><td><strong>纹理滤波预测 (Texture Filtering and Prediction)</strong>：这是一种基于<strong>传统信号处理</strong>的经典方法。它假设在一个小的时间窗口内，物体的运动是匀速直线运动。通过对脉冲进行滤波，估计出物体的运动方向和速度，然后沿着运动轨迹来“填充”和“拉伸”纹理，从而形成清晰的图像。</td><td>• <strong>优点</strong>：计算速度快，不需要训练，对于匀速直线运动的物体效果非常好。<br>• <strong>缺点</strong>：处理变速或旋转等复杂运动时，效果会下降，可能产生错误的纹理拉伸。</td></tr><tr><td><strong>TFI</strong></td><td><strong>纹理滤波插值 (Texture Filtering and Interpolation)</strong>：原理与TFP非常相似，同样基于运动估计和纹理滤波。不同之处在于，TFI更侧重于<strong>插值</strong>，即利用前后时间点的脉冲信息来补全中间缺失的图像帧。</td><td>• <strong>与TFP的区别</strong>：可以看作是TFP的一种变体或改进，两者经常被放在一起比较。它们都属于基于运动补偿的传统方法。<br>• <strong>特点</strong>：同样对高速、匀速运动的物体表现出色。</td></tr><tr><td><strong>TFSTP</strong></td><td><strong>时域滤波与时空可塑性 (Temporal Filtering and Spatio-Temporal Plasticity)</strong>：这是TFP/TFI的<strong>升级版</strong>，引入了<strong>类脑计算</strong>的“短期可塑性 (STP)”概念。它不仅估计运动，还模拟神经元在接收到脉冲信号后，其响应强度会随时间动态变化的特性。</td><td>• <strong>与TFP/TFI的区别</strong>：比TFP/TFI更智能。TFP/TFI像是“刚性”地拉伸纹理，而TFSTP会根据脉冲的“新鲜度”给它们不同的权重，使得重构的图像在运动边缘和纹理细节上更加平滑和自然。<br>• <strong>特点</strong>：融合了传统信号处理和类脑计算的思想。</td></tr><tr><td><strong>SSML</strong></td><td><strong>自监督相互学习 (Self-Supervised Mutual Learning)</strong>：这是一种<strong>无监督的深度学习</strong>方法。它不需要真实的图像作为标签来训练。其核心思想是，从一段脉冲流中重建出两幅图像，一幅只使用奇数时间的脉冲，另一幅只使用偶数时间的脉冲。然后，它强迫这两个“残缺”的重构结果在内容上（如亮度、结构）尽可能地相似。</td><td>• <strong>与前面算法的区别</strong>：完全抛弃了传统的运动估计，改用神经网络端到端地学习如何从脉冲直接生成图像。因为是自监督，所以特别适合处理没有标签的真实世界数据。<br>• <strong>特点</strong>：基于深度学习，效果上限高，能处理复杂场景，但需要训练，计算量也更大。</td></tr><tr><td><strong>SRR</strong></td><td><strong>超分辨率重构 (Super-Resolution Reconstruction)</strong>：这是一种<strong>有监督的深度学习</strong>方法，目标不仅是重构图像，还要<strong>提升图像的分辨率</strong>。它利用脉冲数据极高的时间分辨率，来弥补空间分辨率的不足。其网络模型会学习如何从一段时序的、低分辨率的脉冲中，推理出更高分辨率的图像细节。</td><td>• <strong>与前面算法的区别</strong>：核心目标是“超分”，而不仅仅是“重构”。它需要高分辨率的图像作为训练标签。<br>• <strong>特点</strong>：利用时间信息换取空间信息，可以生成比传感器物理分辨率更高的图像。</td></tr></tbody></table><hr><h3 id="_2-光流估计算法-1个" tabindex="-1"><a class="header-anchor" href="#_2-光流估计算法-1个"><span>2. 光流估计算法 (1个)</span></a></h3><p>这类算法的目标是计算图像中每个像素点的运动矢量，即它在下一帧会移动到哪里。</p><table><thead><tr><th>算法</th><th>核心原理</th></tr></thead><tbody><tr><td><strong>SCFlow</strong></td><td><strong>脉冲到光流的深度学习网络</strong>：这是一种<strong>有监督的深度学习</strong>方法。它设计了一个专门的神经网络，直接学习从输入的脉冲数据块到对应的光流图之间的映射关系。它通常使用模拟器生成的、带有光流真值标签的数据集（如<code>SPIFT</code>）进行训练。</td></tr></tbody></table><hr><h3 id="_3-多目标跟踪算法-1个" tabindex="-1"><a class="header-anchor" href="#_3-多目标跟踪算法-1个"><span>3. 多目标跟踪算法 (1个)</span></a></h3><p>这类算法的目标是在视频序列中检测出多个物体，并为每个物体分配一个唯一的ID，持续追踪它们的运动轨迹。</p><table><thead><tr><th>算法</th><th>核心原理</th></tr></thead><tbody><tr><td><strong>SpikeSORT</strong></td><td><strong>基于脉冲的简单在线实时跟踪 (Spike-based Simple Online and Realtime Tracking)</strong>：这个算法是经典的<strong>SORT跟踪框架</strong>在脉冲数据上的应用和优化。其原理是“<strong>跟踪通过检测 (Tracking-by-Detection)</strong>”。它分为两步：<br>1. <strong>检测</strong>：在每一帧（或每一个时间窗）的脉冲数据中，检测出当前所有物体的位置（通常是边界框）。<br>2. <strong>关联</strong>：使用卡尔曼滤波器预测上一帧中各个物体在当前帧的新位置，然后通过计算预测位置与当前检测到的位置之间的距离（如IoU），将它们匹配起来，从而实现ID的持续分配和轨迹的更新。</td></tr></tbody></table><h3 id="好的老板-根据您提供的这份文档-其中一共示范和介绍了-7-个具体的算法。" tabindex="-1"><a class="header-anchor" href="#好的老板-根据您提供的这份文档-其中一共示范和介绍了-7-个具体的算法。"><span>好的老板，根据您提供的这份文档，其中一共示范和介绍了 7 个具体的算法。</span></a></h3><p>这些算法可以按照任务类型归纳如下：</p><ol><li>纹理重构与超分辨率 (5 个算法) 这部分算法的目标是从脉冲数据中恢复出清晰的图像或视频。</li></ol><p>TFI 算法：用于超高速运动场景的纹理重构 。</p><p>TFP 算法：同样用于超高速运动场景的纹理重构 。</p><p>TFSTP 算法：也是一种用于高速场景的纹理重构算法 。</p><p>SSML 算法：一种用于真实场景重构的神经网络算法 (SSML_ReconNet)，特点是训练时不需要真值（GroundTruth） 。</p><p>SRR 算法：一种超分辨率重构算法 (SRR_model)，可以生成比原始分辨率更高的图像，需要 Pyflow 库来计算光流 。</p><ol start="2"><li>光流估计 (1 个算法) 这类算法用于计算场景中物体的运动速度和方向。</li></ol><p>SCFlow 算法：一个用于从脉冲数据中估计光流的神经网络 。</p><ol start="3"><li>多目标跟踪 (1 个算法) 这类算法用于在场景中同时跟踪多个高速运动的物体。</li></ol><p>SpikeSORT 算法：一个多目标跟踪器，文档中用它在 motVidarReal2020 数据集上进行了演示 。</p><h2 id="想法" tabindex="-1"><a class="header-anchor" href="#想法"><span>想法</span></a></h2><p>黄教授团队的“脉冲视觉”研究从基础理论方面突破，变革了已经沿用两个世纪的传统感光方式，</p><h3 id="使得每个新型感光元器件可以每个像素单独对光子进行捕获-并可以重现拍摄时任意时刻的清晰图像。也就是说-有了脉冲相机-也许以后都不会再谈帧率了-因为拍摄到的信息将无限接近现实的光的运动-其时域采样率为-4-万赫兹-这是以前所有动辄上百万的高帧率相机都无法比拟的。此外-脉冲相机的感光元件可以利用现有的-cmos-制造技术上进行制造-使得成本可以控制得比较低。这项技术可以说意义非凡-前景广泛。" tabindex="-1"><a class="header-anchor" href="#使得每个新型感光元器件可以每个像素单独对光子进行捕获-并可以重现拍摄时任意时刻的清晰图像。也就是说-有了脉冲相机-也许以后都不会再谈帧率了-因为拍摄到的信息将无限接近现实的光的运动-其时域采样率为-4-万赫兹-这是以前所有动辄上百万的高帧率相机都无法比拟的。此外-脉冲相机的感光元件可以利用现有的-cmos-制造技术上进行制造-使得成本可以控制得比较低。这项技术可以说意义非凡-前景广泛。"><span>使得每个新型感光元器件可以每个像素单独对光子进行捕获，并可以重现拍摄时任意时刻的清晰图像。也就是说，有了脉冲相机，也许以后都不会再谈帧率了，因为拍摄到的信息将无限接近现实的光的运动，其时域采样率为 4 万赫兹，这是以前所有动辄上百万的高帧率相机都无法比拟的。此外，脉冲相机的感光元件可以利用现有的 CMOS 制造技术上进行制造，使得成本可以控制得比较低。这项技术可以说意义非凡，前景广泛。</span></a></h3><h3 id="接着-脉冲相机在高铁检修、涡轮停机检修等方面的应用。我比较感兴趣的是基于脉冲的视觉任务。传统计算机视觉基于的图像、视频等基本是用传统相机拍摄的-当然也不排除用特殊成像方式的-在计算机中图像通常以像素矩阵存储" tabindex="-1"><a class="header-anchor" href="#接着-脉冲相机在高铁检修、涡轮停机检修等方面的应用。我比较感兴趣的是基于脉冲的视觉任务。传统计算机视觉基于的图像、视频等基本是用传统相机拍摄的-当然也不排除用特殊成像方式的-在计算机中图像通常以像素矩阵存储"><span>接着，脉冲相机在高铁检修、涡轮停机检修等方面的应用。我比较感兴趣的是基于脉冲的视觉任务。传统计算机视觉基于的图像、视频等基本是用传统相机拍摄的（当然也不排除用特殊成像方式的），在计算机中图像通常以像素矩阵存储，</span></a></h3><h3 id="而脉冲相机拍摄的脉冲序列我认为其实可以看做光子的到达时间序列-提供了更接近物理现实的记录。在脉冲序列上黄教授团队也做了一系列研究-将光流与深度估计、检测跟踪、目标识别等视觉任务基于脉冲序列进行了实现-得到了成倍的速度提升与-sota-的效果。我觉得在脉冲相机的基础上未来可以实现更类脑的神经网络并实现更接近人类视觉能力的方法。" tabindex="-1"><a class="header-anchor" href="#而脉冲相机拍摄的脉冲序列我认为其实可以看做光子的到达时间序列-提供了更接近物理现实的记录。在脉冲序列上黄教授团队也做了一系列研究-将光流与深度估计、检测跟踪、目标识别等视觉任务基于脉冲序列进行了实现-得到了成倍的速度提升与-sota-的效果。我觉得在脉冲相机的基础上未来可以实现更类脑的神经网络并实现更接近人类视觉能力的方法。"><span>而脉冲相机拍摄的脉冲序列我认为其实可以看做光子的到达时间序列，提供了更接近物理现实的记录。在脉冲序列上黄教授团队也做了一系列研究，将光流与深度估计、检测跟踪、目标识别等视觉任务基于脉冲序列进行了实现，得到了成倍的速度提升与 SOTA 的效果。我觉得在脉冲相机的基础上未来可以实现更类脑的神经网络并实现更接近人类视觉能力的方法。</span></a></h3><p>基于脉冲的视觉任务方面，黄教授团队设计并开源了 SpikeCV，即在 OpenCV 的基础上，针对脉冲序列进行了设计，还额外提供了脉冲相机的接口和已经拍摄好的数据集。地址：https://openi.pcl.ac.cn/Cordium/SpikeCV</p><p>有同学问了两个比较关心的问题，即脉冲相机的功耗会不会由于高速采样而很高？以及，脉冲相机拍摄的视频占用存储空间会不会很大？黄教授的回答基本上是，首先功耗确实会比正常大，但可以接受。其次存储由于对脉冲进行了编码，其占用带宽在 Gbps，存储空间需求虽然大但也可以在现有硬件上可以得到满足。</p><p>总之，黄教授的这次演讲干货满满，相信不久的将来可以从前沿研究真正上到我们手机上。</p></div><!--[--><!--]--></div><footer class="vp-page-meta"><!----><div class="vp-meta-item git-info"><div class="vp-meta-item last-updated"><span class="meta-item-label">最近更新: </span><time class="meta-item-info" datetime="2025-08-12T08:03:04.000Z" data-allow-mismatch>2025/8/12 08:03</time></div><div class="vp-meta-item contributors"><span class="meta-item-label">Contributors: </span><span class="meta-item-info"><!--[--><!--[--><span class="contributor" title="email: 2403133073@qq.com">ChenxiMoon</span><!----><!--]--><!--]--></span></div></div></footer><!----><!--[--><!--]--></main><!--]--></div><!--[--><!----><!--]--><!--]--></div>
    <script type="module" src="/assets/app-DIN4ixgB.js" defer></script>
  </body>
</html>
